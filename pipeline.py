# Run KISS-ICP with raw metric depth estimation using Depth Anything V2 for the kitti_360 dataset
# Add Depth_Anything_V2 to the path
import os
import sys
module_path = os.path.abspath(os.path.join('..'))
if module_path not in sys.path:
    sys.path.append(module_path)
    
import numpy as np
import shutil
import pickle
import numpy as np
import torch
import torch.nn.functional as F
from torchvision.transforms import Compose
import torchvision.transforms as transforms
import cv2
import matplotlib
matplotlib.use('agg')  # Or any other X11 back-end
import matplotlib.pyplot as plt

from datasets.afm_depth_estimator_kitti_360 import AFMDepthEstimatorKITTI360Dataset
from afm_kiss_icp.AFMOdometryPipeline import AFMOdometryPipeline
from eval.kiss_icp_eval import run_sequence, print_metrics_per_sequence, print_metrics_table, plot_trajectories

# Redefine Depth Anything V2 image2tensor to use CPU only
from Depth_Anything_V2.metric_depth.depth_anything_v2.dpt import DepthAnythingV2
from Depth_Anything_V2.metric_depth.depth_anything_v2.util.transform import Resize, NormalizeImage, PrepareForNet

print("### Successfully finished imports ###")

class DepthAnythingMetricV2Ours(DepthAnythingV2):
    
    def image2tensor(self, raw_image, input_size=518):        
        transform = Compose([
            Resize(
                width=input_size,
                height=input_size,
                resize_target=False,
                keep_aspect_ratio=True,
                ensure_multiple_of=14,
                resize_method='lower_bound',
                image_interpolation_method=cv2.INTER_CUBIC,
            ),
            NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            PrepareForNet(),
        ])
        
        h, w = raw_image.shape[:2]
        
        # image = cv2.cvtColor(raw_image, cv2.COLOR_BGR2RGB) * 255.0
        
        image = transform({'image': raw_image})['image']
        image = torch.from_numpy(image).unsqueeze(0)
        
        DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
        image = image.to(DEVICE)
        
        return image, (h, w)
    

def main():
    dataset_path = '/storage/group/dataset_mirrors/01_incoming/kitti_360/KITTI-360'
    poses_path = '/storage/group/dataset_mirrors/01_incoming/kitti_360/KITTI-360/data_poses'
    config_file = '../config/KITTI_config.yaml'
    sequences_ids = ['0000', '0002', '0003', '0004', '0005', '0006', '0007', '0009', '0010']

    sequence_to_test = ['0003']     # Dummy variable for testing

    OUTPUT_DIR = './data/kiss_icp_DA_V2_metric/'

    # Create output folder
    # Create output folder if it does not exist
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)

    # Delete results folder generated by KISS-ICP
    shutil.rmtree('results', ignore_errors=True)

    # Create Depth Anything v2 model fine tuned on Virtual KITTI for metric depth
    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
    encoder = 'vitl'  # Use ViTL
    dataset = 'vkitti' # 'hypersim' for indoor model, 'vkitti' for outdoor model
    max_depth = 80 # 20 for indoor model, 80 for outdoor model

    model_configs = {
        'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384],},
        'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768],},
        'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024],}
    }

    depth_anything_v2_metric = DepthAnythingMetricV2Ours(**{**model_configs[encoder], 'max_depth': max_depth})
    depth_anything_v2_metric.load_state_dict(torch.load(f'../Depth_Anything_V2/metric_depth/checkpoints/depth_anything_v2_metric_vkitti_{encoder}.pth', map_location=DEVICE))
    depth_anything_v2_metric.to(DEVICE).eval()

    results = {}
    print("Loaded Depth Anything V2 model for metric depth")
    # Loop over all the sequences
    # for sequence in sequences_ids:
    for sequence in sequence_to_test:
        # Load the dataset for the sequence
        kitti360_sequence = AFMDepthEstimatorKITTI360Dataset(depth_estimator_model=depth_anything_v2_metric, use_sky_masks=True, estimator_mode='metric_depth', data_path=dataset_path, pose_path=poses_path, sequence_id=sequence_to_test[0], mode='lidar', split_path=None, return_scans=False)
        
        def kitti_360_sequence(sequence: int):
            return AFMOdometryPipeline(
                dataset=kitti360_sequence,
                config=config_file,
            )
        run_sequence(kitti_360_sequence, sequence=sequence, results=results)

    # Save all results to files
    print_metrics_per_sequence(results, output_path=os.path.join(OUTPUT_DIR, 'results.txt'))
    plot_trajectories(results, output_path=OUTPUT_DIR)
    plot_trajectories(results, output_path=OUTPUT_DIR, show_correspondances=True, dataset_path=dataset_path, poses_path=poses_path)
    plot_trajectories(results, output_path=OUTPUT_DIR, project_to_XY=True)
    plot_trajectories(results, output_path=OUTPUT_DIR, project_to_XY=True, show_correspondances=True, dataset_path=dataset_path, poses_path=poses_path)
    # Save python dictionary to file
    with open(os.path.join(OUTPUT_DIR, 'results.pkl'), 'wb') as f:
        pickle.dump(results, f)
    
if __name__ == '__main__':
    main()